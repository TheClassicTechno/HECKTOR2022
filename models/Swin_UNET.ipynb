{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!pip install -r https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/requirements-dev.txt\n",
        "!pip install volumentations-3D"
      ],
      "metadata": {
        "id": "JkokcyNEOXL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECt9JKePZ-_d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as tnf\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "\n",
        "from volumentations import *\n",
        "import nibabel as nib\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai import transforms\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Activations,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.utils.enums import MetricReduction\n",
        "from monai.networks.nets import SwinUNETR, UNETR\n",
        "from monai import data\n",
        "from monai.data import decollate_batch\n",
        "from functools import partial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbe8nOJOqa7Y"
      },
      "outputs": [],
      "source": [
        "root_dir = \"/content/gdrive/MyDrive/Summer Programs/HECKTOR2022/DATA/nnUNet_raw_data_base/nnUNet_raw_data/Task501_Hecktor\"\n",
        "batch_size = 1\n",
        "num_workers = 2\n",
        "total_epoch = 200\n",
        "lr = 0.0001\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQawQIiCAwRB"
      },
      "source": [
        "### Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s03LadIjAxQg"
      },
      "outputs": [],
      "source": [
        "model = SwinUNETR(\n",
        "    img_size=(160, 160, 64),\n",
        "    in_channels=2,\n",
        "    out_channels=3,\n",
        "    feature_size=48,\n",
        "    drop_rate=0.0,\n",
        "    attn_drop_rate=0.0,\n",
        "    dropout_path_rate=0.0\n",
        ").to(device)\n",
        "\n",
        "# model = UNETR(\n",
        "#     img_size=(160, 160, 64),\n",
        "#     in_channels=2,\n",
        "#     out_channels=2,\n",
        "#     feature_size=48\n",
        "# ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ZpQ4GRCIff"
      },
      "source": [
        "### Dataset Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0C306HiH0OF"
      },
      "outputs": [],
      "source": [
        "def get_augmentation_train(patch_size):\n",
        "    return Compose([\n",
        "        Rotate((-8, 8), (-8, 8), (-8, 8), p=0.5),\n",
        "        RandomCrop(shape = (160, 160, 64), p = 1.0),\n",
        "        ElasticTransform((0, 0.25), interpolation=2, p=0.1),\n",
        "        #GaussianNoise(var_limit=(0, 5), p=0.2),\n",
        "        #RandomGamma(gamma_limit=(0.5, 1.5), p=0.2),\n",
        "    ], p=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qjoj1zBB7Tn"
      },
      "outputs": [],
      "source": [
        "def get_augmentation_test(patch_size):\n",
        "    return Compose([\n",
        "        RandomCrop(shape = (160, 160, 64), p = 1.0)\n",
        "    ], p=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXyzgtRuVGj1"
      },
      "outputs": [],
      "source": [
        "def tissue_thresholding(img):\n",
        "  min, max = -175, 275\n",
        "  img = np.clip(img, min, max)\n",
        "  img = (img - min) / (max - min)\n",
        "  return img\n",
        "def pet_rescaling(img):\n",
        "  min, max = np.min(img), np.max(img)\n",
        "  img = (img - min) / (max - min)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "612x70J2Ggvf"
      },
      "outputs": [],
      "source": [
        "def load_data(root_dir, filename, split, aug):\n",
        "  filename = filename.upper()\n",
        "  dir_img = os.path.join(root_dir, \"imagesTr\")\n",
        "  dir_seg = os.path.join(root_dir, \"labelsTr\")\n",
        "  if split == 'test':\n",
        "    dir_img = os.path.join(root_dir, \"imagesTs\")\n",
        "    dir_seg = os.path.join(root_dir, \"labelsTs\")\n",
        "  filename_ct = os.path.join(dir_img,  filename[:-7] + '_0000.nii.gz')\n",
        "  filename_pt = os.path.join(dir_img, filename[:-7] + '_0001.nii.gz')\n",
        "  filename_mask = os.path.join(dir_seg, filename[:-7] + '.nii.gz')\n",
        "  ct_img = np.expand_dims(nib.load(filename_ct).get_fdata(), axis = 3)\n",
        "  pet_img = np.expand_dims(nib.load(filename_pt).get_fdata(), axis = 3)\n",
        "  mask = nib.load(filename_mask).get_fdata()\n",
        "  # mask_1 = np.expand_dims((mask==1.).astype(np.float32), axis = 3)\n",
        "  # mask_2 = np.expand_dims((mask==2.).astype(np.float32), axis = 3)\n",
        "  # mask = np.concatenate([mask_0, mask_1, mask_2], axis=3)\n",
        "  mask = keras.utils.to_categorical(\n",
        "   mask, num_classes=3, dtype='float32'\n",
        "  )\n",
        "  img = np.concatenate((ct_img, pet_img), axis = 3)\n",
        "  if aug: \n",
        "    if(split == 'train'):\n",
        "      aug = get_augmentation_train((160, 160, 64))\n",
        "    else:\n",
        "      aug = get_augmentation_test((160, 160, 64))\n",
        "    data = {'image': img, 'mask': mask}\n",
        "    aug_data = aug(**data)\n",
        "    img, mask = aug_data['image'], aug_data['mask']\n",
        "    img, mask = img.transpose(3 , 0, 1, 2).astype(np.float32), mask.transpose(3, 0, 1, 2).astype(np.float32)\n",
        "    return img, mask\n",
        "  else:\n",
        "    img, mask = img.transpose(3 , 0, 1, 2), mask.transpose(3, 0, 1, 2)\n",
        "    return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydfO2dPmaLJc"
      },
      "outputs": [],
      "source": [
        "class HEKDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train'):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.image_paths = []\n",
        "        # Load data index\n",
        "        if split == 'train':\n",
        "          for path in os.listdir(os.path.join(root_dir, \"labelsTr\")):\n",
        "                  self.image_paths.append(path)\n",
        "        else:\n",
        "          for path in os.listdir(os.path.join(root_dir, \"labelsTs\")):\n",
        "              self.image_paths.append(path)\n",
        "        print('Succesfully loaded {} dataset.'.format(split) + ' '*50)\n",
        "            \n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_paths[idx]\n",
        "        return load_data(self.root_dir, filename, self.split, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW1xlFwpKQOU",
        "outputId": "af040526-79d7-49be-ed89-d5f2f47c34d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Succesfully loaded train dataset.                                                  \n",
            "Succesfully loaded test dataset.                                                  \n"
          ]
        }
      ],
      "source": [
        "train_set = HEKDataset(root_dir, split='train')\n",
        "test_set = HEKDataset(root_dir, split='test')\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True,)\n",
        "val_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBLN-jUuq3HE"
      },
      "source": [
        "###Metrics + Losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv9TSsqerfIT"
      },
      "outputs": [],
      "source": [
        "def compute_dice_coef(inputs, targets, smooth = 1e-7):\n",
        "  #inputs = tnf.softmax(inputs)\n",
        "  inputs = inputs.view(-1)\n",
        "  targets = targets.view(-1)\n",
        "  \n",
        "  intersection = (inputs * targets).sum()                            \n",
        "  dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "  return (2.*intersection), (inputs.sum() + targets.sum())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9s1kzDQcVcX"
      },
      "outputs": [],
      "source": [
        "def flatten(tensor):\n",
        "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
        "    The shapes are transformed as follows:\n",
        "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
        "    \"\"\"\n",
        "    tensor = tensor[:, 1:, :, :, :]\n",
        "    # number of channels\n",
        "    C = tensor.size(1)\n",
        "    # new axis order\n",
        "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
        "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
        "    transposed = tensor.permute(axis_order)\n",
        "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
        "    return transposed.contiguous().view(C, -1)\n",
        "    \n",
        "def compute_per_channel_dice(input, target, epsilon=1e-6, weight=None):\n",
        "    \"\"\"\n",
        "    Computes DiceCoefficient as defined in https://arxiv.org/abs/1606.04797 given  a multi channel input and target.\n",
        "    Assumes the input is a normalized probability, e.g. a result of Sigmoid or Softmax function.\n",
        "    Args:\n",
        "         input (torch.Tensor): NxCxSpatial input tensor\n",
        "         target (torch.Tensor): NxCxSpatial target tensor\n",
        "         epsilon (float): prevents division by zero\n",
        "         weight (torch.Tensor): Cx1 tensor of weight per channel/class\n",
        "    \"\"\"\n",
        "\n",
        "    # input and target shapes must match\n",
        "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
        "\n",
        "    input = flatten(input)\n",
        "    target = flatten(target)\n",
        "    target = target.float()\n",
        "\n",
        "    # compute per channel Dice Coefficient\n",
        "    intersect = (input * target).sum(-1)\n",
        "    if weight is not None:\n",
        "        intersect = weight * intersect\n",
        "\n",
        "    # here we can use standard dice (input + target).sum(-1) or extension (see V-Net) (input^2 + target^2).sum(-1)\n",
        "    denominator = (input * input).sum(-1) + (target * target).sum(-1)\n",
        "    return 2 * (intersect / denominator.clamp(min=epsilon))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clC5QptScvbg"
      },
      "outputs": [],
      "source": [
        "class _AbstractDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for different implementations of Dice loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, normalization='sigmoid'):\n",
        "        super(_AbstractDiceLoss, self).__init__()\n",
        "        self.register_buffer('weight', weight)\n",
        "        # The output from the network during training is assumed to be un-normalized probabilities and we would\n",
        "        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n",
        "        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n",
        "        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n",
        "        # output, just specify `normalization=Softmax`\n",
        "        assert normalization in ['sigmoid', 'softmax', 'none']\n",
        "        if normalization == 'sigmoid':\n",
        "            self.normalization = nn.Sigmoid()\n",
        "        elif normalization == 'softmax':\n",
        "            self.normalization = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            self.normalization = lambda x: x\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        # actual Dice score computation; to be implemented by the subclass\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # get probabilities from logits\n",
        "        input = self.normalization(input)\n",
        "\n",
        "        # compute per channel Dice coefficient\n",
        "        per_channel_dice = self.dice(input, target, weight=self.weight)\n",
        "\n",
        "        # average Dice score across all channels/classes\n",
        "        return 1. - torch.mean(per_channel_dice)\n",
        "\n",
        "\n",
        "class DiceLoss(_AbstractDiceLoss):\n",
        "    \"\"\"Computes Dice Loss according to https://arxiv.org/abs/1606.04797.\n",
        "    For multi-class segmentation `weight` parameter can be used to assign different weights per class.\n",
        "    The input to the loss function is assumed to be a logit and will be normalized by the Sigmoid function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, normalization='sigmoid'):\n",
        "        super().__init__(weight, normalization)\n",
        "\n",
        "    def dice(self, input, target, weight):\n",
        "        return compute_per_channel_dice(input, target, weight=self.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSDj3-DyNplV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Helper function to enable loss function to be flexibly used for \n",
        "# both 2D or 3D image segmentation - source: https://github.com/frankkramer-lab/MIScnn\n",
        "\n",
        "def identify_axis(shape):\n",
        "    # Three dimensional\n",
        "    if len(shape) == 5 : return [2,3,4]\n",
        "\n",
        "    \n",
        "    # Exception - Unknown\n",
        "    else : raise ValueError('Metric: Shape of tensor is neither 2D or 3D.')\n",
        "\n",
        "\n",
        "class AsymmetricFocalLoss(nn.Module):\n",
        "    \"\"\"For Imbalanced datasets\n",
        "    Parameters\n",
        "    ----------\n",
        "    delta : float, optional\n",
        "        controls weight given to false positive and false negatives, by default 0.25\n",
        "    gamma : float, optional\n",
        "        Focal Tversky loss' focal parameter controls degree of down-weighting of easy examples, by default 2.0\n",
        "    epsilon : float, optional\n",
        "        clip values to prevent division by zero error\n",
        "    \"\"\"\n",
        "    def __init__(self, delta=0.7, gamma=2., epsilon=1e-07):\n",
        "        super(AsymmetricFocalLoss, self).__init__()\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
        "        cross_entropy = -y_true * torch.log(y_pred)\n",
        "        \n",
        "\t# Calculate losses separately for each class, only suppressing background class\n",
        "        back_ce = torch.pow(1 - y_pred[:,0,:,:], self.gamma) * cross_entropy[:,0,:,:]\n",
        "        back_ce =  (1 - self.delta) * back_ce\n",
        "\n",
        "        fore_ce = cross_entropy[:,1:,:,:]\n",
        "        fore_ce = self.delta * fore_ce\n",
        "        #print(fore_ce.size())\n",
        "        loss = torch.mean(torch.sum(torch.stack([back_ce, fore_ce[:,0], fore_ce[:,1]], axis=-1), axis=-1))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AsymmetricFocalTverskyLoss(nn.Module):\n",
        "    \"\"\"This is the implementation for binary segmentation.\n",
        "    Parameters\n",
        "    ----------\n",
        "    delta : float, optional\n",
        "        controls weight given to false positive and false negatives, by default 0.7\n",
        "    gamma : float, optional\n",
        "        focal parameter controls degree of down-weighting of easy examples, by default 0.75\n",
        "    smooth : float, optional\n",
        "        smooithing constant to prevent division by 0 errors, by default 0.000001\n",
        "    epsilon : float, optional\n",
        "        clip values to prevent division by zero error\n",
        "    \"\"\"\n",
        "    def __init__(self, delta=0.7, gamma=0.75, epsilon=1e-07):\n",
        "        super(AsymmetricFocalTverskyLoss, self).__init__()\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip values to prevent division by zero error\n",
        "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
        "        axis = identify_axis(y_true.size())\n",
        "\n",
        "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)     \n",
        "        tp = torch.sum(y_true * y_pred, axis=axis)\n",
        "        fn = torch.sum(y_true * (1-y_pred), axis=axis)\n",
        "        fp = torch.sum((1-y_true) * y_pred, axis=axis)\n",
        "        dice_class = (tp + self.epsilon)/(tp + self.delta*fn + (1-self.delta)*fp + self.epsilon)\n",
        "\n",
        "        # Calculate losses separately for each class, only enhancing foreground class\n",
        "        back_dice = (1-dice_class[:,0]) \n",
        "        fore_dice = (1-dice_class[:,1:]) * torch.pow(1-dice_class[:,1:], -self.gamma) \n",
        "        #print(fore_dice.size())\n",
        "        # Average class scores\n",
        "        loss = torch.mean(torch.stack([back_dice,fore_dice[:, 0], fore_dice[:, 1]], axis=-1))\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "class AsymmetricUnifiedFocalLoss(nn.Module):\n",
        "    \"\"\"The Unified Focal loss is a new compound loss function that unifies Dice-based and cross entropy-based loss functions into a single framework.\n",
        "    Parameters\n",
        "    ----------\n",
        "    weight : float, optional\n",
        "        represents lambda parameter and controls weight given to asymmetric Focal Tversky loss and asymmetric Focal loss, by default 0.5\n",
        "    delta : float, optional\n",
        "        controls weight given to each class, by default 0.6\n",
        "    gamma : float, optional\n",
        "        focal parameter controls the degree of background suppression and foreground enhancement, by default 0.5\n",
        "    epsilon : float, optional\n",
        "        clip values to prevent division by zero error\n",
        "    \"\"\"\n",
        "    def __init__(self, weight=0.5, delta=0.6, gamma=0.2):\n",
        "        super(AsymmetricUnifiedFocalLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.delta = delta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "      # Obtain Asymmetric Focal Tversky loss\n",
        "      activation = nn.Softmax(dim = 1)\n",
        "      y_pred = activation(y_pred)\n",
        "      asymmetric_ftl = AsymmetricFocalTverskyLoss(delta=self.delta, gamma=self.gamma)(y_pred, y_true)\n",
        "\n",
        "      # Obtain Asymmetric Focal loss\n",
        "      asymmetric_fl = AsymmetricFocalLoss(delta=self.delta, gamma=self.gamma)(y_pred, y_true)\n",
        "\n",
        "      # Return weighted sum of Asymmetrical Focal loss and Asymmetric Focal Tversky loss\n",
        "      if self.weight is not None:\n",
        "        return (self.weight * asymmetric_ftl) + ((1-self.weight) * asymmetric_fl)  \n",
        "      else:\n",
        "        return asymmetric_ftl + asymmetric_fl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE8ytOSfHhC9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# input = torch.randn((1, 2, 160, 160, 64)).to(device)\n",
        "# output = model(input)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "#seg_loss = DiceLoss(norma1ization = 'softmax')\n",
        "seg_loss = AsymmetricUnifiedFocalLoss()\n",
        "#print([e.shape for e in output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7pJwgX1Jw7j"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXK8bkA3kNXJ",
        "outputId": "11ffe1c3-b6bd-4b8d-ada1-328315cdac51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Summer Programs/HECKTOR2022/Saved Models/SWIN_UNETR_new.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtmHmo40hVmi"
      },
      "outputs": [],
      "source": [
        "num_train = len(train_set)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(0.2 * num_train))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpl73bdt3-Y-"
      },
      "outputs": [],
      "source": [
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_subsampler = SubsetRandomSampler(train_idx)\n",
        "val_subsampler = SubsetRandomSampler(valid_idx)\n",
        "train_loader = DataLoader(\n",
        "                      train_set, \n",
        "                      batch_size=batch_size, \n",
        "                      num_workers=num_workers,\n",
        "                      pin_memory=True, sampler=train_subsampler)\n",
        "val_loader = DataLoader(\n",
        "                      train_set,\n",
        "                      batch_size=batch_size, \n",
        "                      num_workers=num_workers,\n",
        "                      pin_memory=True, sampler=val_subsampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvGPUNAOAbqk"
      },
      "outputs": [],
      "source": [
        "nb_train_batches = len(train_loader)\n",
        "nb_val_batches = len(val_loader)\n",
        "nb_iter = 0\n",
        "best_val_DC = 0.\n",
        "iters = list(range(1, 10))\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "train_accuracy=[]\n",
        "val_accuracy=[]\n",
        "curr_epoch = 57\n",
        "best_val_DC = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uofmQ3pMJrwt",
        "outputId": "e5c1d7e9-ea9b-400f-db38-3d92ea8d5e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58, iter 453/453, loss 0.115703\n",
            "Validation iter 70/70\n",
            "Epoch 58, Class 1 Train DC: 0.836261, Class 2 Train DC: 0.809865, Val Loss:0.182402, Class 1 Val DC:0.802431, Class 2 Val DC:0.662214\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 59, iter 453/453, loss 0.113858\n",
            "Validation iter 70/70\n",
            "Epoch 59, Class 1 Train DC: 0.841507, Class 2 Train DC: 0.813089, Val Loss:0.179423, Class 1 Val DC:0.803774, Class 2 Val DC:0.664127\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 60, iter 453/453, loss 0.112844\n",
            "Validation iter 70/70\n",
            "Epoch 60, Class 1 Train DC: 0.842795, Class 2 Train DC: 0.816055, Val Loss:0.178404, Class 1 Val DC:0.805139, Class 2 Val DC:0.637140\n",
            "__________________________________________________\n",
            "Epoch 61, iter 453/453, loss 0.113486\n",
            "Validation iter 70/70\n",
            "Epoch 61, Class 1 Train DC: 0.842510, Class 2 Train DC: 0.814173, Val Loss:0.183365, Class 1 Val DC:0.789121, Class 2 Val DC:0.661424\n",
            "__________________________________________________\n",
            "Epoch 62, iter 453/453, loss 0.110600\n",
            "Validation iter 70/70\n",
            "Epoch 62, Class 1 Train DC: 0.847414, Class 2 Train DC: 0.820075, Val Loss:0.179353, Class 1 Val DC:0.802281, Class 2 Val DC:0.661225\n",
            "__________________________________________________\n",
            "Epoch 63, iter 453/453, loss 0.110872\n",
            "Validation iter 70/70\n",
            "Epoch 63, Class 1 Train DC: 0.847805, Class 2 Train DC: 0.815402, Val Loss:0.182964, Class 1 Val DC:0.791493, Class 2 Val DC:0.673283\n",
            "__________________________________________________\n",
            "Epoch 64, iter 453/453, loss 0.110310\n",
            "Validation iter 70/70\n",
            "Epoch 64, Class 1 Train DC: 0.849262, Class 2 Train DC: 0.820618, Val Loss:0.180208, Class 1 Val DC:0.798828, Class 2 Val DC:0.670653\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 65, iter 453/453, loss 0.112313\n",
            "Validation iter 70/70\n",
            "Epoch 65, Class 1 Train DC: 0.844143, Class 2 Train DC: 0.814736, Val Loss:0.181069, Class 1 Val DC:0.798301, Class 2 Val DC:0.655324\n",
            "__________________________________________________\n",
            "Epoch 66, iter 453/453, loss 0.108736\n",
            "Validation iter 70/70\n",
            "Epoch 66, Class 1 Train DC: 0.847959, Class 2 Train DC: 0.821437, Val Loss:0.184506, Class 1 Val DC:0.753387, Class 2 Val DC:0.656460\n",
            "__________________________________________________\n",
            "Epoch 67, iter 453/453, loss 0.108948\n",
            "Validation iter 70/70\n",
            "Epoch 67, Class 1 Train DC: 0.849414, Class 2 Train DC: 0.820425, Val Loss:0.182627, Class 1 Val DC:0.796714, Class 2 Val DC:0.656787\n",
            "__________________________________________________\n",
            "Epoch 68, iter 453/453, loss 0.108355\n",
            "Validation iter 70/70\n",
            "Epoch 68, Class 1 Train DC: 0.848980, Class 2 Train DC: 0.824946, Val Loss:0.183761, Class 1 Val DC:0.779964, Class 2 Val DC:0.651640\n",
            "__________________________________________________\n",
            "Epoch 69, iter 453/453, loss 0.108418\n",
            "Validation iter 70/70\n",
            "Epoch 69, Class 1 Train DC: 0.851842, Class 2 Train DC: 0.822147, Val Loss:0.182098, Class 1 Val DC:0.802902, Class 2 Val DC:0.668200\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 70, iter 453/453, loss 0.107756\n",
            "Validation iter 70/70\n",
            "Epoch 70, Class 1 Train DC: 0.852206, Class 2 Train DC: 0.826018, Val Loss:0.182468, Class 1 Val DC:0.803257, Class 2 Val DC:0.655831\n",
            "__________________________________________________\n",
            "Epoch 71, iter 453/453, loss 0.107296\n",
            "Validation iter 70/70\n",
            "Epoch 71, Class 1 Train DC: 0.850797, Class 2 Train DC: 0.823795, Val Loss:0.189009, Class 1 Val DC:0.789029, Class 2 Val DC:0.658739\n",
            "__________________________________________________\n",
            "Epoch 72, iter 453/453, loss 0.107690\n",
            "Validation iter 70/70\n",
            "Epoch 72, Class 1 Train DC: 0.853148, Class 2 Train DC: 0.823688, Val Loss:0.185440, Class 1 Val DC:0.798500, Class 2 Val DC:0.650705\n",
            "__________________________________________________\n",
            "Epoch 73, iter 453/453, loss 0.107355\n",
            "Validation iter 70/70\n",
            "Epoch 73, Class 1 Train DC: 0.851867, Class 2 Train DC: 0.826957, Val Loss:0.181034, Class 1 Val DC:0.787872, Class 2 Val DC:0.660256\n",
            "__________________________________________________\n",
            "Epoch 74, iter 453/453, loss 0.107189\n",
            "Validation iter 70/70\n",
            "Epoch 74, Class 1 Train DC: 0.852861, Class 2 Train DC: 0.827500, Val Loss:0.180391, Class 1 Val DC:0.803517, Class 2 Val DC:0.661975\n",
            "__________________________________________________\n",
            "Epoch 75, iter 453/453, loss 0.105890\n",
            "Validation iter 70/70\n",
            "Epoch 75, Class 1 Train DC: 0.857589, Class 2 Train DC: 0.824684, Val Loss:0.178996, Class 1 Val DC:0.799826, Class 2 Val DC:0.659487\n",
            "__________________________________________________\n",
            "Epoch 76, iter 453/453, loss 0.106283\n",
            "Validation iter 70/70\n",
            "Epoch 76, Class 1 Train DC: 0.854591, Class 2 Train DC: 0.826824, Val Loss:0.180589, Class 1 Val DC:0.816574, Class 2 Val DC:0.653089\n",
            "__________________________________________________\n",
            "Epoch 77, iter 453/453, loss 0.105896\n",
            "Validation iter 70/70\n",
            "Epoch 77, Class 1 Train DC: 0.856600, Class 2 Train DC: 0.828287, Val Loss:0.178831, Class 1 Val DC:0.811980, Class 2 Val DC:0.647925\n",
            "__________________________________________________\n",
            "Epoch 78, iter 453/453, loss 0.105474\n",
            "Validation iter 70/70\n",
            "Epoch 78, Class 1 Train DC: 0.856517, Class 2 Train DC: 0.831752, Val Loss:0.184744, Class 1 Val DC:0.801966, Class 2 Val DC:0.657621\n",
            "__________________________________________________\n",
            "Epoch 79, iter 453/453, loss 0.105266\n",
            "Validation iter 70/70\n",
            "Epoch 79, Class 1 Train DC: 0.857231, Class 2 Train DC: 0.829729, Val Loss:0.181879, Class 1 Val DC:0.800673, Class 2 Val DC:0.670964\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 80, iter 453/453, loss 0.105950\n",
            "Validation iter 70/70\n",
            "Epoch 80, Class 1 Train DC: 0.852897, Class 2 Train DC: 0.826239, Val Loss:0.188978, Class 1 Val DC:0.775343, Class 2 Val DC:0.645567\n",
            "__________________________________________________\n",
            "Epoch 81, iter 453/453, loss 0.104209\n",
            "Validation iter 70/70\n",
            "Epoch 81, Class 1 Train DC: 0.859806, Class 2 Train DC: 0.830532, Val Loss:0.177151, Class 1 Val DC:0.792014, Class 2 Val DC:0.640997\n",
            "__________________________________________________\n",
            "Epoch 82, iter 453/453, loss 0.104857\n",
            "Validation iter 70/70\n",
            "Epoch 82, Class 1 Train DC: 0.860145, Class 2 Train DC: 0.830152, Val Loss:0.184542, Class 1 Val DC:0.799302, Class 2 Val DC:0.637595\n",
            "__________________________________________________\n",
            "Epoch 83, iter 453/453, loss 0.103387\n",
            "Validation iter 70/70\n",
            "Epoch 83, Class 1 Train DC: 0.861089, Class 2 Train DC: 0.833821, Val Loss:0.183134, Class 1 Val DC:0.804452, Class 2 Val DC:0.661516\n",
            "__________________________________________________\n",
            "Epoch 84, iter 453/453, loss 0.103422\n",
            "Validation iter 70/70\n",
            "Epoch 84, Class 1 Train DC: 0.859508, Class 2 Train DC: 0.832752, Val Loss:0.180950, Class 1 Val DC:0.788272, Class 2 Val DC:0.653149\n",
            "__________________________________________________\n",
            "Epoch 85, iter 453/453, loss 0.103672\n",
            "Validation iter 70/70\n",
            "Epoch 85, Class 1 Train DC: 0.860447, Class 2 Train DC: 0.834096, Val Loss:0.186599, Class 1 Val DC:0.787904, Class 2 Val DC:0.669568\n",
            "__________________________________________________\n",
            "Epoch 86, iter 453/453, loss 0.104389\n",
            "Validation iter 70/70\n",
            "Epoch 86, Class 1 Train DC: 0.860458, Class 2 Train DC: 0.833430, Val Loss:0.188071, Class 1 Val DC:0.790725, Class 2 Val DC:0.645163\n",
            "__________________________________________________\n",
            "Epoch 87, iter 453/453, loss 0.102633\n",
            "Validation iter 70/70\n",
            "Epoch 87, Class 1 Train DC: 0.861454, Class 2 Train DC: 0.836593, Val Loss:0.181934, Class 1 Val DC:0.796834, Class 2 Val DC:0.659784\n",
            "__________________________________________________\n",
            "Epoch 88, iter 453/453, loss 0.104670\n",
            "Validation iter 70/70\n",
            "Epoch 88, Class 1 Train DC: 0.859070, Class 2 Train DC: 0.834264, Val Loss:0.180901, Class 1 Val DC:0.790580, Class 2 Val DC:0.679995\n",
            "__________________________________________________\n",
            "Epoch 89, iter 453/453, loss 0.102657\n",
            "Validation iter 70/70\n",
            "Epoch 89, Class 1 Train DC: 0.863273, Class 2 Train DC: 0.834033, Val Loss:0.184397, Class 1 Val DC:0.794243, Class 2 Val DC:0.663492\n",
            "__________________________________________________\n",
            "Epoch 90, iter 453/453, loss 0.102727\n",
            "Validation iter 70/70\n",
            "Epoch 90, Class 1 Train DC: 0.862035, Class 2 Train DC: 0.833595, Val Loss:0.183739, Class 1 Val DC:0.803603, Class 2 Val DC:0.657212\n",
            "__________________________________________________\n",
            "Epoch 91, iter 453/453, loss 0.102257\n",
            "Validation iter 70/70\n",
            "Epoch 91, Class 1 Train DC: 0.862814, Class 2 Train DC: 0.833526, Val Loss:0.185700, Class 1 Val DC:0.799460, Class 2 Val DC:0.653620\n",
            "__________________________________________________\n",
            "Epoch 92, iter 453/453, loss 0.100963\n",
            "Validation iter 70/70\n",
            "Epoch 92, Class 1 Train DC: 0.866845, Class 2 Train DC: 0.838064, Val Loss:0.182977, Class 1 Val DC:0.781513, Class 2 Val DC:0.649956\n",
            "__________________________________________________\n",
            "Epoch 93, iter 453/453, loss 0.102047\n",
            "Validation iter 70/70\n",
            "Epoch 93, Class 1 Train DC: 0.866512, Class 2 Train DC: 0.836560, Val Loss:0.180003, Class 1 Val DC:0.785496, Class 2 Val DC:0.647239\n",
            "__________________________________________________\n",
            "Epoch 94, iter 453/453, loss 0.101539\n",
            "Validation iter 70/70\n",
            "Epoch 94, Class 1 Train DC: 0.868545, Class 2 Train DC: 0.838714, Val Loss:0.179130, Class 1 Val DC:0.796165, Class 2 Val DC:0.672039\n",
            "__________________________________________________\n",
            "Epoch 95, iter 453/453, loss 0.100408\n",
            "Validation iter 70/70\n",
            "Epoch 95, Class 1 Train DC: 0.865349, Class 2 Train DC: 0.840372, Val Loss:0.187846, Class 1 Val DC:0.795820, Class 2 Val DC:0.652641\n",
            "__________________________________________________\n",
            "Epoch 96, iter 453/453, loss 0.099536\n",
            "Validation iter 70/70\n",
            "Epoch 96, Class 1 Train DC: 0.868588, Class 2 Train DC: 0.842522, Val Loss:0.183416, Class 1 Val DC:0.801161, Class 2 Val DC:0.651197\n",
            "__________________________________________________\n",
            "Epoch 97, iter 453/453, loss 0.098989\n",
            "Validation iter 70/70\n",
            "Epoch 97, Class 1 Train DC: 0.870746, Class 2 Train DC: 0.843918, Val Loss:0.185185, Class 1 Val DC:0.799480, Class 2 Val DC:0.649300\n",
            "__________________________________________________\n",
            "Epoch 98, iter 453/453, loss 0.101088\n",
            "Validation iter 70/70\n",
            "Epoch 98, Class 1 Train DC: 0.866736, Class 2 Train DC: 0.838986, Val Loss:0.187428, Class 1 Val DC:0.788447, Class 2 Val DC:0.665441\n",
            "__________________________________________________\n",
            "Epoch 99, iter 453/453, loss 0.101932\n",
            "Validation iter 70/70\n",
            "Epoch 99, Class 1 Train DC: 0.867350, Class 2 Train DC: 0.838387, Val Loss:0.182860, Class 1 Val DC:0.809469, Class 2 Val DC:0.630360\n",
            "__________________________________________________\n",
            "Epoch 100, iter 453/453, loss 0.101384\n",
            "Validation iter 70/70\n",
            "Epoch 100, Class 1 Train DC: 0.865307, Class 2 Train DC: 0.842280, Val Loss:0.183534, Class 1 Val DC:0.789043, Class 2 Val DC:0.655075\n",
            "__________________________________________________\n",
            "Epoch 101, iter 453/453, loss 0.099386\n",
            "Validation iter 70/70\n",
            "Epoch 101, Class 1 Train DC: 0.870262, Class 2 Train DC: 0.844943, Val Loss:0.187493, Class 1 Val DC:0.802687, Class 2 Val DC:0.633132\n",
            "__________________________________________________\n",
            "Epoch 102, iter 453/453, loss 0.099499\n",
            "Validation iter 70/70\n",
            "Epoch 102, Class 1 Train DC: 0.868565, Class 2 Train DC: 0.845897, Val Loss:0.178451, Class 1 Val DC:0.799466, Class 2 Val DC:0.648279\n",
            "__________________________________________________\n",
            "Epoch 103, iter 453/453, loss 0.100104\n",
            "Validation iter 70/70\n",
            "Epoch 103, Class 1 Train DC: 0.869523, Class 2 Train DC: 0.840874, Val Loss:0.179117, Class 1 Val DC:0.801427, Class 2 Val DC:0.659703\n",
            "__________________________________________________\n",
            "Epoch 104, iter 453/453, loss 0.098920\n",
            "Validation iter 70/70\n",
            "Epoch 104, Class 1 Train DC: 0.872097, Class 2 Train DC: 0.842504, Val Loss:0.190929, Class 1 Val DC:0.799447, Class 2 Val DC:0.630155\n",
            "__________________________________________________\n",
            "Epoch 105, iter 453/453, loss 0.098408\n",
            "Validation iter 70/70\n",
            "Epoch 105, Class 1 Train DC: 0.872381, Class 2 Train DC: 0.846282, Val Loss:0.187477, Class 1 Val DC:0.801491, Class 2 Val DC:0.636872\n",
            "__________________________________________________\n",
            "Epoch 106, iter 453/453, loss 0.098474\n",
            "Validation iter 70/70\n",
            "Epoch 106, Class 1 Train DC: 0.871637, Class 2 Train DC: 0.846887, Val Loss:0.185890, Class 1 Val DC:0.793969, Class 2 Val DC:0.635623\n",
            "__________________________________________________\n",
            "Epoch 107, iter 453/453, loss 0.098618\n",
            "Validation iter 70/70\n",
            "Epoch 107, Class 1 Train DC: 0.871579, Class 2 Train DC: 0.845238, Val Loss:0.183226, Class 1 Val DC:0.788619, Class 2 Val DC:0.665117\n",
            "__________________________________________________\n",
            "Epoch 108, iter 453/453, loss 0.098588\n",
            "Validation iter 70/70\n",
            "Epoch 108, Class 1 Train DC: 0.870302, Class 2 Train DC: 0.847246, Val Loss:0.181864, Class 1 Val DC:0.788445, Class 2 Val DC:0.658840\n",
            "__________________________________________________\n",
            "Epoch 109, iter 453/453, loss 0.099225\n",
            "Validation iter 70/70\n",
            "Epoch 109, Class 1 Train DC: 0.870250, Class 2 Train DC: 0.849901, Val Loss:0.185186, Class 1 Val DC:0.793835, Class 2 Val DC:0.657598\n",
            "__________________________________________________\n",
            "Epoch 110, iter 453/453, loss 0.098823\n",
            "Validation iter 70/70\n",
            "Epoch 110, Class 1 Train DC: 0.871176, Class 2 Train DC: 0.846351, Val Loss:0.180473, Class 1 Val DC:0.796365, Class 2 Val DC:0.658438\n",
            "__________________________________________________\n",
            "Epoch 111, iter 453/453, loss 0.098058\n",
            "Validation iter 70/70\n",
            "Epoch 111, Class 1 Train DC: 0.871947, Class 2 Train DC: 0.848252, Val Loss:0.186390, Class 1 Val DC:0.799840, Class 2 Val DC:0.644098\n",
            "__________________________________________________\n",
            "Epoch 112, iter 453/453, loss 0.098540\n",
            "Validation iter 70/70\n",
            "Epoch 112, Class 1 Train DC: 0.871876, Class 2 Train DC: 0.848927, Val Loss:0.187413, Class 1 Val DC:0.780642, Class 2 Val DC:0.644172\n",
            "__________________________________________________\n",
            "Epoch 113, iter 453/453, loss 0.097114\n",
            "Validation iter 70/70\n",
            "Epoch 113, Class 1 Train DC: 0.872901, Class 2 Train DC: 0.849676, Val Loss:0.181386, Class 1 Val DC:0.810502, Class 2 Val DC:0.649741\n",
            "__________________________________________________\n",
            "Epoch 114, iter 453/453, loss 0.096482\n",
            "Validation iter 70/70\n",
            "Epoch 114, Class 1 Train DC: 0.875688, Class 2 Train DC: 0.850439, Val Loss:0.183523, Class 1 Val DC:0.792773, Class 2 Val DC:0.654217\n",
            "__________________________________________________\n",
            "Epoch 115, iter 453/453, loss 0.096205\n",
            "Validation iter 70/70\n",
            "Epoch 115, Class 1 Train DC: 0.875279, Class 2 Train DC: 0.849889, Val Loss:0.183770, Class 1 Val DC:0.803643, Class 2 Val DC:0.644357\n",
            "__________________________________________________\n",
            "Epoch 116, iter 453/453, loss 0.096567\n",
            "Validation iter 70/70\n",
            "Epoch 116, Class 1 Train DC: 0.875844, Class 2 Train DC: 0.851392, Val Loss:0.181372, Class 1 Val DC:0.794804, Class 2 Val DC:0.662080\n",
            "__________________________________________________\n",
            "Epoch 117, iter 453/453, loss 0.098325\n",
            "Validation iter 70/70\n",
            "Epoch 117, Class 1 Train DC: 0.871719, Class 2 Train DC: 0.848221, Val Loss:0.183291, Class 1 Val DC:0.802807, Class 2 Val DC:0.668353\n",
            "__________________________________________________\n",
            "Epoch 118, iter 453/453, loss 0.095477\n",
            "Validation iter 70/70\n",
            "Epoch 118, Class 1 Train DC: 0.876573, Class 2 Train DC: 0.853946, Val Loss:0.183864, Class 1 Val DC:0.802399, Class 2 Val DC:0.628895\n",
            "__________________________________________________\n",
            "Epoch 119, iter 453/453, loss 0.095615\n",
            "Validation iter 70/70\n",
            "Epoch 119, Class 1 Train DC: 0.878533, Class 2 Train DC: 0.853954, Val Loss:0.178905, Class 1 Val DC:0.803070, Class 2 Val DC:0.671906\n",
            "Best validation DC reached on epoch. Saved model weights.\n",
            "__________________________________________________\n",
            "Epoch 120, iter 453/453, loss 0.096445\n",
            "Validation iter 70/70\n",
            "Epoch 120, Class 1 Train DC: 0.875368, Class 2 Train DC: 0.850354, Val Loss:0.184034, Class 1 Val DC:0.808470, Class 2 Val DC:0.659378\n",
            "__________________________________________________\n",
            "Epoch 121, iter 453/453, loss 0.095993\n",
            "Validation iter 70/70\n",
            "Epoch 121, Class 1 Train DC: 0.876490, Class 2 Train DC: 0.852804, Val Loss:0.184637, Class 1 Val DC:0.794448, Class 2 Val DC:0.646836\n",
            "__________________________________________________\n",
            "Epoch 122, iter 453/453, loss 0.095883\n",
            "Validation iter 70/70\n",
            "Epoch 122, Class 1 Train DC: 0.879445, Class 2 Train DC: 0.851321, Val Loss:0.189512, Class 1 Val DC:0.793764, Class 2 Val DC:0.659855\n",
            "__________________________________________________\n",
            "Epoch 123, iter 453/453, loss 0.096328\n",
            "Validation iter 70/70\n",
            "Epoch 123, Class 1 Train DC: 0.876357, Class 2 Train DC: 0.853109, Val Loss:0.182850, Class 1 Val DC:0.794126, Class 2 Val DC:0.662133\n",
            "__________________________________________________\n",
            "Epoch 124, iter 453/453, loss 0.094647\n",
            "Validation iter 70/70\n",
            "Epoch 124, Class 1 Train DC: 0.879807, Class 2 Train DC: 0.854572, Val Loss:0.185079, Class 1 Val DC:0.795257, Class 2 Val DC:0.645584\n",
            "__________________________________________________\n",
            "Epoch 125, iter 453/453, loss 0.096100\n",
            "Validation iter 70/70\n",
            "Epoch 125, Class 1 Train DC: 0.876871, Class 2 Train DC: 0.852725, Val Loss:0.187262, Class 1 Val DC:0.804062, Class 2 Val DC:0.662581\n",
            "__________________________________________________\n",
            "Epoch 126, iter 453/453, loss 0.095357\n",
            "Validation iter 70/70\n",
            "Epoch 126, Class 1 Train DC: 0.877935, Class 2 Train DC: 0.855179, Val Loss:0.185214, Class 1 Val DC:0.804613, Class 2 Val DC:0.641218\n",
            "__________________________________________________\n",
            "Epoch 127, iter 453/453, loss 0.094829\n",
            "Validation iter 70/70\n",
            "Epoch 127, Class 1 Train DC: 0.877533, Class 2 Train DC: 0.855924, Val Loss:0.189548, Class 1 Val DC:0.792189, Class 2 Val DC:0.631557\n",
            "__________________________________________________\n",
            "Epoch 128, iter 453/453, loss 0.095895\n",
            "Validation iter 70/70\n",
            "Epoch 128, Class 1 Train DC: 0.876979, Class 2 Train DC: 0.855827, Val Loss:0.186035, Class 1 Val DC:0.781818, Class 2 Val DC:0.656470\n",
            "__________________________________________________\n",
            "Epoch 129, iter 453/453, loss 0.095910\n",
            "Validation iter 70/70\n",
            "Epoch 129, Class 1 Train DC: 0.877211, Class 2 Train DC: 0.855042, Val Loss:0.182660, Class 1 Val DC:0.805361, Class 2 Val DC:0.666625\n",
            "__________________________________________________\n",
            "Epoch 130, iter 453/453, loss 0.094766\n",
            "Validation iter 70/70\n",
            "Epoch 130, Class 1 Train DC: 0.879141, Class 2 Train DC: 0.855562, Val Loss:0.185070, Class 1 Val DC:0.798615, Class 2 Val DC:0.662203\n",
            "__________________________________________________\n",
            "Epoch 131, iter 453/453, loss 0.093501\n",
            "Validation iter 70/70\n",
            "Epoch 131, Class 1 Train DC: 0.881543, Class 2 Train DC: 0.858757, Val Loss:0.183733, Class 1 Val DC:0.801457, Class 2 Val DC:0.657827\n",
            "__________________________________________________\n",
            "Epoch 132, iter 453/453, loss 0.094378\n",
            "Validation iter 70/70\n",
            "Epoch 132, Class 1 Train DC: 0.879799, Class 2 Train DC: 0.855438, Val Loss:0.186358, Class 1 Val DC:0.802136, Class 2 Val DC:0.657138\n",
            "__________________________________________________\n",
            "Epoch 133, iter 453/453, loss 0.094158\n",
            "Validation iter 70/70\n",
            "Epoch 133, Class 1 Train DC: 0.880022, Class 2 Train DC: 0.857919, Val Loss:0.186004, Class 1 Val DC:0.787671, Class 2 Val DC:0.645018\n",
            "__________________________________________________\n",
            "Epoch 134, iter 453/453, loss 0.094158\n",
            "Validation iter 70/70\n",
            "Epoch 134, Class 1 Train DC: 0.880509, Class 2 Train DC: 0.857219, Val Loss:0.191660, Class 1 Val DC:0.791871, Class 2 Val DC:0.651742\n",
            "__________________________________________________\n",
            "Epoch 135, iter 453/453, loss 0.097026\n",
            "Validation iter 70/70\n",
            "Epoch 135, Class 1 Train DC: 0.873176, Class 2 Train DC: 0.849903, Val Loss:0.188641, Class 1 Val DC:0.784158, Class 2 Val DC:0.663815\n",
            "__________________________________________________\n",
            "Epoch 136, iter 453/453, loss 0.094509\n",
            "Validation iter 70/70\n",
            "Epoch 136, Class 1 Train DC: 0.880884, Class 2 Train DC: 0.854716, Val Loss:0.186833, Class 1 Val DC:0.798354, Class 2 Val DC:0.648913\n",
            "__________________________________________________\n",
            "Epoch 137, iter 453/453, loss 0.092193\n",
            "Validation iter 70/70\n",
            "Epoch 137, Class 1 Train DC: 0.881876, Class 2 Train DC: 0.860932, Val Loss:0.185410, Class 1 Val DC:0.794081, Class 2 Val DC:0.656577\n",
            "__________________________________________________\n",
            "Epoch 138, iter 453/453, loss 0.093405\n",
            "Validation iter 70/70\n",
            "Epoch 138, Class 1 Train DC: 0.881260, Class 2 Train DC: 0.858239, Val Loss:0.182292, Class 1 Val DC:0.785104, Class 2 Val DC:0.660397\n",
            "__________________________________________________\n",
            "Epoch 139, iter 453/453, loss 0.094592\n",
            "Validation iter 70/70\n",
            "Epoch 139, Class 1 Train DC: 0.882299, Class 2 Train DC: 0.856193, Val Loss:0.190925, Class 1 Val DC:0.799930, Class 2 Val DC:0.635819\n",
            "__________________________________________________\n",
            "Epoch 140, iter 453/453, loss 0.094830\n",
            "Validation iter 70/70\n",
            "Epoch 140, Class 1 Train DC: 0.880864, Class 2 Train DC: 0.857078, Val Loss:0.186582, Class 1 Val DC:0.788324, Class 2 Val DC:0.658048\n",
            "__________________________________________________\n",
            "Epoch 141, iter 453/453, loss 0.094173\n",
            "Validation iter 70/70\n",
            "Epoch 141, Class 1 Train DC: 0.880313, Class 2 Train DC: 0.856950, Val Loss:0.183530, Class 1 Val DC:0.793087, Class 2 Val DC:0.656113\n",
            "__________________________________________________\n",
            "Epoch 142, iter 453/453, loss 0.093295\n",
            "Validation iter 70/70\n",
            "Epoch 142, Class 1 Train DC: 0.882855, Class 2 Train DC: 0.860304, Val Loss:0.181028, Class 1 Val DC:0.802651, Class 2 Val DC:0.654299\n",
            "__________________________________________________\n",
            "Epoch 143, iter 453/453, loss 0.093867\n",
            "Validation iter 70/70\n",
            "Epoch 143, Class 1 Train DC: 0.881397, Class 2 Train DC: 0.860614, Val Loss:0.183024, Class 1 Val DC:0.792260, Class 2 Val DC:0.662910\n",
            "__________________________________________________\n",
            "Epoch 144, iter 453/453, loss 0.092946\n",
            "Validation iter 70/70\n",
            "Epoch 144, Class 1 Train DC: 0.883589, Class 2 Train DC: 0.860918, Val Loss:0.183659, Class 1 Val DC:0.801346, Class 2 Val DC:0.654313\n",
            "__________________________________________________\n",
            "Epoch 145, iter 453/453, loss 0.092725\n",
            "Validation iter 70/70\n",
            "Epoch 145, Class 1 Train DC: 0.884207, Class 2 Train DC: 0.860517, Val Loss:0.185569, Class 1 Val DC:0.797819, Class 2 Val DC:0.658841\n",
            "__________________________________________________\n",
            "Epoch 146, iter 453/453, loss 0.092772\n",
            "Validation iter 70/70\n",
            "Epoch 146, Class 1 Train DC: 0.883119, Class 2 Train DC: 0.860237, Val Loss:0.188435, Class 1 Val DC:0.798178, Class 2 Val DC:0.650449\n",
            "__________________________________________________\n",
            "Epoch 147, iter 453/453, loss 0.093893\n",
            "Validation iter 70/70\n",
            "Epoch 147, Class 1 Train DC: 0.880895, Class 2 Train DC: 0.857168, Val Loss:0.184757, Class 1 Val DC:0.780732, Class 2 Val DC:0.667313\n",
            "__________________________________________________\n",
            "Epoch 148, iter 453/453, loss 0.092553\n",
            "Validation iter 70/70\n",
            "Epoch 148, Class 1 Train DC: 0.883462, Class 2 Train DC: 0.860651, Val Loss:0.188242, Class 1 Val DC:0.804405, Class 2 Val DC:0.632645\n",
            "__________________________________________________\n",
            "Epoch 149, iter 453/453, loss 0.092705\n",
            "Validation iter 70/70\n",
            "Epoch 149, Class 1 Train DC: 0.883335, Class 2 Train DC: 0.862836, Val Loss:0.185011, Class 1 Val DC:0.803413, Class 2 Val DC:0.663828\n",
            "__________________________________________________\n",
            "Epoch 150, iter 453/453, loss 0.092234\n",
            "Validation iter 70/70\n",
            "Epoch 150, Class 1 Train DC: 0.882680, Class 2 Train DC: 0.861940, Val Loss:0.181451, Class 1 Val DC:0.798542, Class 2 Val DC:0.669254\n",
            "__________________________________________________\n",
            "Epoch 151, iter 453/453, loss 0.091121\n",
            "Validation iter 70/70\n",
            "Epoch 151, Class 1 Train DC: 0.886071, Class 2 Train DC: 0.864355, Val Loss:0.181666, Class 1 Val DC:0.803617, Class 2 Val DC:0.662338\n",
            "__________________________________________________\n",
            "Epoch 152, iter 453/453, loss 0.092739\n",
            "Validation iter 70/70\n",
            "Epoch 152, Class 1 Train DC: 0.882018, Class 2 Train DC: 0.861935, Val Loss:0.185057, Class 1 Val DC:0.801609, Class 2 Val DC:0.664156\n",
            "__________________________________________________\n",
            "Epoch 153, iter 453/453, loss 0.091653\n",
            "Validation iter 70/70\n",
            "Epoch 153, Class 1 Train DC: 0.884004, Class 2 Train DC: 0.864909, Val Loss:0.184775, Class 1 Val DC:0.801226, Class 2 Val DC:0.639746\n",
            "__________________________________________________\n",
            "Epoch 154, iter 453/453, loss 0.091564\n",
            "Validation iter 70/70\n",
            "Epoch 154, Class 1 Train DC: 0.885418, Class 2 Train DC: 0.865043, Val Loss:0.182913, Class 1 Val DC:0.808965, Class 2 Val DC:0.652667\n",
            "__________________________________________________\n",
            "Epoch 155, iter 453/453, loss 0.092020\n",
            "Validation iter 70/70\n",
            "Epoch 155, Class 1 Train DC: 0.883187, Class 2 Train DC: 0.863606, Val Loss:0.185815, Class 1 Val DC:0.800444, Class 2 Val DC:0.660445\n",
            "__________________________________________________\n",
            "Epoch 156, iter 453/453, loss 0.091500\n",
            "Validation iter 70/70\n",
            "Epoch 156, Class 1 Train DC: 0.885993, Class 2 Train DC: 0.863769, Val Loss:0.186221, Class 1 Val DC:0.802491, Class 2 Val DC:0.645201\n",
            "__________________________________________________\n",
            "Epoch 157, iter 453/453, loss 0.091932\n",
            "Validation iter 70/70\n",
            "Epoch 157, Class 1 Train DC: 0.883654, Class 2 Train DC: 0.864345, Val Loss:0.185849, Class 1 Val DC:0.798506, Class 2 Val DC:0.656543\n",
            "__________________________________________________\n",
            "Epoch 158, iter 453/453, loss 0.092108\n",
            "Validation iter 70/70\n",
            "Epoch 158, Class 1 Train DC: 0.885900, Class 2 Train DC: 0.862822, Val Loss:0.185647, Class 1 Val DC:0.792543, Class 2 Val DC:0.656768\n",
            "__________________________________________________\n",
            "Epoch 159, iter 453/453, loss 0.090624\n",
            "Validation iter 70/70\n",
            "Epoch 159, Class 1 Train DC: 0.886036, Class 2 Train DC: 0.866011, Val Loss:0.185742, Class 1 Val DC:0.792224, Class 2 Val DC:0.663539\n",
            "__________________________________________________\n",
            "Epoch 160, iter 453/453, loss 0.091444\n",
            "Validation iter 70/70\n",
            "Epoch 160, Class 1 Train DC: 0.886120, Class 2 Train DC: 0.862980, Val Loss:0.188319, Class 1 Val DC:0.783033, Class 2 Val DC:0.661721\n",
            "__________________________________________________\n",
            "Epoch 161, iter 453/453, loss 0.090532\n",
            "Validation iter 70/70\n",
            "Epoch 161, Class 1 Train DC: 0.888408, Class 2 Train DC: 0.865922, Val Loss:0.184181, Class 1 Val DC:0.787321, Class 2 Val DC:0.661717\n",
            "__________________________________________________\n",
            "Epoch 162, iter 453/453, loss 0.091049\n",
            "Validation iter 70/70\n",
            "Epoch 162, Class 1 Train DC: 0.887051, Class 2 Train DC: 0.862556, Val Loss:0.188359, Class 1 Val DC:0.780093, Class 2 Val DC:0.631701\n",
            "__________________________________________________\n",
            "Epoch 163, iter 453/453, loss 0.091051\n",
            "Validation iter 70/70\n",
            "Epoch 163, Class 1 Train DC: 0.889340, Class 2 Train DC: 0.866699, Val Loss:0.187829, Class 1 Val DC:0.787513, Class 2 Val DC:0.630579\n",
            "__________________________________________________\n",
            "Epoch 164, iter 453/453, loss 0.091009\n",
            "Validation iter 70/70\n",
            "Epoch 164, Class 1 Train DC: 0.887457, Class 2 Train DC: 0.864100, Val Loss:0.183728, Class 1 Val DC:0.793267, Class 2 Val DC:0.666053\n",
            "__________________________________________________\n",
            "Epoch 165, iter 21/453, loss 0.057383"
          ]
        }
      ],
      "source": [
        "while curr_epoch  < total_epoch:\n",
        "    train_loss, val_loss = 0., 0.\n",
        "    train_dsc_1, val_dsc_1 = 0., 0.\n",
        "    train_dsc_2, val_dsc_2 = 0., 0.\n",
        "    intersection1, union1 = 0,0\n",
        "    intersection2, union2 = 0,0\n",
        "    ############\n",
        "    # TRAINING #\n",
        "    ############\n",
        "    model.train()\n",
        "    train_data = iter(train_loader)\n",
        "    for k in range(nb_train_batches):\n",
        "        imgs, seg_gts = train_data.next()\n",
        "        #print(imgs.size(), seg_gts.size(), device)\n",
        "        imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(imgs)\n",
        "        #print(logits.size, imgs.size)\n",
        "        loss = seg_loss(logits, seg_gts)\n",
        " \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() / nb_train_batches\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = torch.argmax(logits, axis = 1)\n",
        "            pred_1 = (preds==1.).type(torch.float32)\n",
        "            pred_2 = (preds==2.).type(torch.float32)\n",
        "\n",
        "            gt_1 = seg_gts[:,1,:,:, :].type(torch.int8)\n",
        "            gt_2 = seg_gts[:,2,:,:, :].type(torch.int8)\n",
        "            inter1, u1 = compute_dice_coef(pred_1, gt_1)\n",
        "            inter2, u2 = compute_dice_coef(pred_2, gt_2)\n",
        "            intersection1 += inter1\n",
        "            intersection2 += inter2\n",
        "            union1 += u1\n",
        "            union2 += u2\n",
        "\n",
        "        train_dsc_1 = intersection1 / union1\n",
        "        train_dsc_2 = intersection2 / union2\n",
        "\n",
        "            \n",
        "        # Increase iterations\n",
        "        nb_iter += 1\n",
        "        \n",
        "        print('\\rEpoch {}, iter {}/{}, loss {:.6f}'.format(curr_epoch+1, k+1, nb_train_batches, loss.item()), \n",
        "              end='')\n",
        "    print('\\rEpoch {}, iter {}/{}, loss {:.6f}'.format(curr_epoch+1, k+1, nb_train_batches, train_loss), \n",
        "              end='')\n",
        "    print()\n",
        "    intersection1, union1 = 0,0\n",
        "    intersection2, union2 = 0,0\n",
        "    ##############\n",
        "    # VALIDATION #\n",
        "    ##############\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_data = iter(val_loader)\n",
        "        for k in range(nb_val_batches):\n",
        "            # Loads data\n",
        "            imgs, seg_gts = val_data.next()\n",
        "            imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(imgs)\n",
        "            val_loss += seg_loss(logits, seg_gts).item() / nb_val_batches\n",
        "            \n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Std out\n",
        "            print('\\rValidation iter {}/{}'.format(k+1, nb_val_batches), end='')\n",
        "            \n",
        "            # Compute segmentation metric\n",
        "            preds = torch.argmax(logits, axis = 1)\n",
        "            pred_1 = (preds==1.).type(torch.float32)\n",
        "            pred_2 = (preds==2.).type(torch.float32)\n",
        "\n",
        "            #pred_1 = (logits[:,0,:,:, :]>=0.5).type(torch.int8).cpu().to(device)\n",
        "      \n",
        "            #pred_2 = (logits[:,1,:,:, :]>=0.5).type(torch.int8).cpu().to(device)\n",
        "  \n",
        "            gt_1 = seg_gts[:,1,:,:, :].type(torch.int8)\n",
        "            gt_2 = seg_gts[:,2,:,:, :].type(torch.int8)\n",
        "            dsc_1 = compute_dice_coef(pred_1, gt_1)\n",
        "            dsc_2 = compute_dice_coef(pred_2, gt_2)\n",
        "\n",
        "            inter1, u1 = compute_dice_coef(pred_1, gt_1)\n",
        "            inter2, u2 = compute_dice_coef(pred_2, gt_2)\n",
        "            intersection1 += inter1\n",
        "            intersection2 += inter2\n",
        "            union1 += u1\n",
        "            union2 += u2\n",
        "\n",
        "        val_dsc_1 = intersection1 / union1\n",
        "        val_dsc_2 = intersection2 / union2\n",
        "    print('\\nEpoch {}, Class 1 Train DC: {:.6f}, Class 2 Train DC: {:.6f}, Val Loss:{:.6f}, Class 1 Val DC:{:.6f}, Class 2 Val DC:{:.6f}'.format(curr_epoch + 1, train_dsc_1, train_dsc_2, val_loss, val_dsc_1, val_dsc_2))\n",
        "    if val_dsc_1 + val_dsc_2 > best_val_DC:\n",
        "        torch.save(model.state_dict(), '/content/gdrive/MyDrive/Summer Programs/HECKTOR2022/Saved Models/SWIN_UNETR_final.pth')\n",
        "        best_val_DC = val_dsc_1 + val_dsc_2\n",
        "        print('Best validation DC reached on epoch. Saved model weights.')\n",
        "    print('_'*50)\n",
        "        \n",
        "    # End of epoch\n",
        "    curr_epoch  += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6j3FthPL_8n"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvIa7objL_I4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "def show_mri(img, mask):\n",
        "    for i in range(img.shape[3]):\n",
        "        fig = plt.figure(figsize=(10, 7))\n",
        "        fig.add_subplot(1, 2, 1)\n",
        "        plt.axis(False)\n",
        "        plt.imshow(img[0, :, :, i], cmap='gray')\n",
        "        fig.add_subplot(1, 2, 2)\n",
        "        plt.axis(False)\n",
        "        plt.imshow(mask[0, :, :, i], cmap='gray')\n",
        "        clear_output(wait=True)\n",
        "        plt.show()\n",
        "def show_mri_slice(img, mask, slice, pred = None):\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    if(pred is  None):\n",
        "      fig.add_subplot(1, 2, 1)\n",
        "      plt.axis(False)\n",
        "      plt.imshow(img[0, :, :, slice], cmap='gray')\n",
        "      fig.add_subplot(1, 2, 2)\n",
        "      plt.axis(False)\n",
        "      plt.imshow(mask[0, :, :, slice], cmap='gray')\n",
        "    else:\n",
        "      fig.add_subplot(1, 3, 1)\n",
        "      plt.axis(False)\n",
        "      plt.imshow(img[0, :, :, slice], cmap='gray')\n",
        "      fig.add_subplot(1, 3, 2)\n",
        "      plt.axis(False)\n",
        "      plt.imshow(mask[0, :, :, slice], cmap='gray')\n",
        "      fig.add_subplot(1, 3, 3)\n",
        "      plt.axis(False)\n",
        "      plt.imshow(pred[0, :, :, slice], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU-3ph3FGr-q"
      },
      "outputs": [],
      "source": [
        "img, mask = load_data(root_dir, \"HEK_001.nii.gz\", 'train', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwzez9YHV9GQ"
      },
      "outputs": [],
      "source": [
        "pred = np.concatenate([np.expand_dims(pred_1, axis = 0), np.expand_dims(pred_2, axis = 0)], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdTLlSB2WOaV"
      },
      "outputs": [],
      "source": [
        "print(img.shape, mask.shape, pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybUtMayrVVvY"
      },
      "outputs": [],
      "source": [
        "show_mri_slice(img, mask, 15, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4R_TKInVXyz"
      },
      "outputs": [],
      "source": [
        "show_mri(img, mask)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}